###存疑

LSTM
GRU
LRN




###激活函数
https://blog.csdn.net/qq_20909377/article/details/79133981

hardsigmoid

关于各种归一化 方法这篇文章说的不错


https://blog.csdn.net/liuxiao214/article/details/81037416
https://blog.csdn.net/qq_39124762/article/details/82082705


Abs
Acos
Acosh
Add
And
ArgMax
ArgMin
Asin
Asinh
Atan
Atanh

##AveragePool
池化操作
param：kernel shape ，stride，padding，auto_pad,ceil_mode,count_include_pad 


##BatchNormalization
批标准化 ，配合mini_batch SGD使用 
每个神经元需要学习scale和shift。Normalization的对象是同一批的同神经元上的数据
https://www.cnblogs.com/guoyaohua/p/8724433.html

Cast

Ceil

##Clip
将输入挤压到某个区间。

##Compress
param：axis
input:x,condition
根据condition 从 x的axis维度里面选取

##Concat
param: axis
input: tensor[]
从axis维把tensors结合在一起

##Constant
##ConstantOfShape
##Conv

param：kernel shape,dilations,group ，stride，padding，auto_pad ,
需要注意 dilation这个参数。如果不为1,说明使用的是扩张卷积（空洞卷积分）。详见https://blog.csdn.net/Quincuntial/article/details/78743033

扩展卷积在保持参数个数不变的情况下增大了卷积核的感受野

##ConvInteger
防止溢出版本的Conv 。但是好像不支持 bias




##ConvTranspose
逆卷积
https://www.zhihu.com/question/43609045?sort=created



Cos

Cosh

##DepthToSpace

###其实spacetodepth就是折叠操作，depthtospace 可以看作逆操作（拼接操作）。本质在于以深度换宽度

transpose+ reshape

将一个较多通道的特征变成较少通道的特征。 
经历了4->reshape [b, blocksize, blocksize, $\frac{c}{blocksize^2}$ , h, w ]->6->
transpose[0, 3, 4, 1, 5, 2]- >
reshape [b, $\frac{c}{blocksize^2}$, h * blocksize, w * blocksize] ->4
三个操作

和<a href="#1"> SpaceToDepth </a>是对称操作

##DequantizeLinear
<a id="2"></a>


Div

##Dropout

随机失活，注意训练时候会dropout，但测试的时候不会
幸存者会被放大1/（1-p)          
p是失活概率

y=f(wx+b)->y=f(dropout(w)x+b)
代表y和x之间加个drop 层 
注意 是 权值被 dropout



Elu

Equal

##Erf
https://baike.baidu.com/item/%E8%AF%AF%E5%B7%AE%E5%87%BD%E6%95%B0/5890875?fr=aladdin、
误差函数
Exp

##Expand
inputs:T,shape


遵循broadcast的expand 。
shape的长度可以大于T

EyeLike

##Flatten
Flattens the input tensor into a 2D matrix. If input tensor has shape (d_0, d_1, ... d_n) then the output will have shape (d_0 X d_1 ... d_(axis-1), d_axis X d_(axis+1) ... X dn).

Floor

##GRU
pass

##Gather
相当于numpy.take
~~~
Ni, Nk = a.shape[:axis], a.shape[axis+1:]
Nj = indices.shape
for ii in ndindex(Ni):
    for jj in ndindex(Nj):
    	for kk in ndindex(Nk):
        	out[ii + jj + kk] = a[ii + (indices[jj],) + kk]
~~~
以上解释来自numpy.take 。  numpy.take 还支持不需要axis。


##Gemm
Y = alpha * A' * B' + beta * C
A,B可以转置，C可以广播

##GlobalAveragePool
直接对通道内求平均（输入dimensions可以大于等于4）

相当于numpy.average(a,axis=(最后两个index) ,keepdim=True)




##GlobalLpPool
params:p
求通道内的p范树
b=np.linalg.norm(a,p,(最后两个index),keepdims=True)

##GlobalMaxPool
相当于numpy.max(a,axis=(最后两个index) ,keepdim=True)
Greater

##HardSigmoid
activation


##Hardmax
hardmax 最大值得1,其他为0 
对给定axis施加


##Identity

copy input

##If
onnx自己做控制流程的


##InstanceNormalization

y = scale * (x - mean) / sqrt(variance + epsilon) + B, 
where mean and variance are computed per instance per channel.

scale ,x,B都是 输入 


IsNaN

LRN

LSTM

LeakyRelu

Less

Log

##LogSoftmax
param:axis
对axis做logsoftmax


Loop

##LpNormalization
p范树正则化


LpPool

##MatMul
对A，B最低的两个维度做matmul .需要保证 A，B前面的维度相同


MatMulInteger

##Max
求最大值，支持广播


##MaxPool
同average pool


##MaxRoiPool



params:
pooled_shape : 两个数字，确定输出shape
spatial_scale : 

Inputs

X :
rois :N*5   [[batch_id, x1, y1, x2, y2], ...].

roi: region of interest!



##MaxUnpool


Mean

Min

Mul

Multinomial

Neg

##NonZero
需要注意下标的排列方式
https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html

Not
##OneHot
params:axis default=-1  -1表示在最后一个维度产生 one hot

input:
indices ：最后一维表示 label
  
depth ：类别数
values [off,on]  off表示冷值，on 表示热值


用来产生 one hot 向量，输出的维度比 indices少一维


Orfire
PRelu
##Pad
input:
x ,
pad[x1_begin, x2_begin...x1_end, x2_end,...],
value =0

在各个维度上 以value 补齐 x。 若 pad中有负数，表示 删除 



Pow
QLinearConv
QLinearMatMul
##QuantizeLinear
该操作是 <a href="#2" >dequantizelinear</a>的逆向操作


##RNN
##RandomNormal
根据shape 生成 正态分布 ,参数 mean ,scale

RandomNormalLike
##RandomUniform
根据shape 生成 uniform分布,参数 low,high

RandomUniformLike


##Reciprocal
y=1/x elementwise

#下面这几个操作都是reduce
##ReduceL1 ReduceL2
params:axes ,keepdims
对给定axes 求L1,L2 norm .可以决定是否keepdims
axes是axis的复数

##ReduceLogSum ReduceLogSumExp ReduceMax ReduceMean ReduceSumSquare



#ReduceMin
#ReduceProd
#ReduceSum

Relu
Reshape
##Resize
params:mode确定插值的策略
input:x ,scales  
scales确定每个维度的膨胀系数.如果 大于1 ,需要插值策略
注意这是一个和numpy.resize不一样的操作


#Scan
onnx的操作


##Scatter
param: axis
input:x,indices,update

indices和x  rank相同.
除了axis那一维,其他唯独的 shape 应当相同


indices和update shape相同

根据 indices把X中的数据更新为update. 返回新的Y


##Selu
激活函数,据说效果比 batchNormalization好


Shape
##Shrink
If x < -lambd, y = x + bias; If x > lambd, y = x - bias; Otherwise, y = 0.

Sigmoid
Sign
Sin
Sinh
Size
#Slice
参考 numpy的slice
https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html
相较于 numpy,貌似不支持用 None做 占位符 的 shape 变化 


Softmax
Softplus
Softsign
##<a id="1">SpaceToDepth </a>

##Split
params:axis ,split
根据axis将对应维度按照split长度 分开  sum(split)应当等于对应axis的长度


Sqrt
##Squeeze
除去单维

##StringNormalizer
做停词和大小写转换的


Sub
##Sum
注意和reducesum的区别就行了

Tan
Tanh
##TfIdfVectorizer
TF-IDF算法

ThresholdedRelu
##Tile
input:
x,
repeats:一维tensor.长度等于rank(x)
对x的每个维度 重复 repeat次.


TopK
Transpose
Unsqueeze
Upsample
#Where
参考numpy.where
https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html

Xor
