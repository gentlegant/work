

##激活函数: 
###激活函数一般都是输入输出同shape 
elu HardSigmoid LeakyRelu RELU prelu relu selu
sigmoid sign softplus softsign ThresholdedRelu
##One-Elementwise:

###输入输出shape相同,并且关系是一对一的
###激活函数都属于这个类别
Asin   Asinh Atan Atanh acos acosh
abs sin cos tan tanh  sinh cosh
Ceil clip(夹)  floor
ERF EXP
DequantizeLinear QuantizeLinear
identity
IsNaN
LOG
LOG_SOFTMAX和softmax(并不是一一映射)
NEG
Not
Reciprocal(y=1/x)
Shrink
sqrt

##Two-elementwise:
###两个输入,一个输出,shape 相同,一对一  支持 广播
add and div Equal greater LESS mul or pow sub xor or

##N-elementwise:
### n个输入,一个输出,shape相同,一对一,支持广播
max mean min sum

##reduce操作: axes  
### 可以对多个axis同时 reduce
ReduceL1 ReduceL2 ReduceLogsum  ReduceLogSumExp ReduceMax ReduceMean  ReduceMin   ReduceProd ReduceSumSquare
##神经网络操作:

dropout

##图像操作:
###其实就是tensor的变换操作.折叠和反折叠
depthToSPACE,spacetodepth

##基本操作:
argmax argmin shape size topk

##卷积系列:
###需要注意pad ,kernel ,stride等..
AveragePool conv convInteger  convtranspose
globalaveragepoo GlobalMaxPooll  LpPool maxpool
maxroipool(目标检测)
maxunpool
QLinearConv
pad

##Normaliztion:
https://blog.csdn.net/liuxiao214/article/details/81037416
https://blog.csdn.net/qq_39124762/article/details/82082705
BatchNormalization InstanceNormalization
LpNormalization  MeanVarianceNormalization
##特殊操作:

cast(类型转换)
constant,constantofshape (生产常数)

##工厂:
eyelike(规定了输入只能是2D,不知道为什么)
Multinomial RandomNormal RandomNormallike 
RandomUniform RandomUniformlike

##变换操作:
compress
concat
expand
flatten(展平成二维(默认第一维是batch,所以故意这样))
reshape
resize(注意不是 numpy的resize)
split
depthToSPACE,spacetodepth
squeeze unsqueeze
Tile
transpose
unsample
##取数操作:
gather scatter slice Where
##查数操作:
nonzero

##LSTM系列:
GRU lstm

##矩阵操作:
gemm  MATMUL matmulInteger QLinearMatMul

##结构操作:
IF
LOOP
scan

##其他:
hardmax
LRN(ALEXNET)
onehot
RNN
StringNormalizer
TfIdfVectorizer
